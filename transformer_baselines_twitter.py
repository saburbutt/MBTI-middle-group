# -*- coding: utf-8 -*-
"""Transformer_baselines_Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXx2DGHPNQv_qAWD228d6_JP6sAbbk1m
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install tensorboardx
!pip install simpletransformers

import csv
import os
import re
import time
import pickle
import string
import spacy
import nltk
import warnings
import random

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
'''import tensorflow as tf

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer'''
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score,fbeta_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.pipeline import Pipeline
#from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
'''from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import text_to_word_sequence
from keras.preprocessing import sequence
from keras import backend
from keras import backend as K
from keras import models
from keras.models import Sequential
from keras.layers import Dense,LSTM, Dropout, Flatten, Embedding, Bidirectional, GlobalAveragePooling1D
from keras.layers import Conv1D,Conv2D, MaxPooling2D, MaxPooling1D, GlobalMaxPooling1D
#from keras.optimizers import Adam,Adadelta,Adagrad,Adamax
from keras.regularizers import l2,l1
from keras.callbacks import CSVLogger
#from keras.utils import to_categorical
from keras.datasets import mnist
from keras.utils.vis_utils import model_to_dot
#from keras.layers.normalization import BatchNormalization
from keras.layers import Conv1D,Conv2D, MaxPooling2D, MaxPooling1D
from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
from IPython.display import SVG
from scipy.stats import norm
from gensim import models
from gensim.models import Word2Vec
from gensim.models.wrappers import FastText
from gensim.models.fasttext import FastText#, load_facebook_vectors
from gensim.models.keyedvectors import KeyedVectors'''
from tqdm import tqdm
from nltk.corpus import wordnet
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, PorterStemmer ,WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')


warnings.filterwarnings("ignore")

#f =  open("/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/2000gb.all", 'rb')
#full_text = f.readlines()
#f.close()
#decode('string_escape')
#for text in full_text[0:1]:
#    print(str(text))
#    print(type(text))
#    print("----------------------------------------------")

"""#Data Statistics"""

# Read TSV file into DataFrame
df = pd.read_table('/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/2000gb.all', sep='\t', encoding='latin-1', header=None,)
print(df)

df

# Save it only one time
# df.to_csv("/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/Personality_twitter_unclean.csv", index=False)

# adding column name to the respective columns
df.columns =['MBTI', 'Gender', 'Number', 'Text']

df["MBTI"].unique()

df[["MBTI", "Gender"]].describe()

# counting unique values
df['MBTI'].value_counts()

print(df.shape)

"""#Data transformatrion

!wget https://nlp.stanford.edu/data/glove.840B.300d.zip

!unzip /content/glove.840B.300d.zip

from gensim.scripts.glove2word2vec import glove2word2vec
glove2word2vec(glove_input_file="/content/glove.840B.300d.txt", word2vec_output_file="gensim_glove_vectors.txt")

!cp '/content/gensim_glove_vectors.txt' '/content/drive/MyDrive/Thesis-Ambiversion'


from gensim.models.keyedvectors import KeyedVectors
glove_model = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/Thesis-Ambiversion/gensim_glove_vectors.txt", binary=False)

def cov_check(texts, pretrained_vocab):

    oov_dict = {}
    oov_num = 0
    cov_num = 0


    for text in texts:
        for word in text.split():
            try:
              vector = pretrained_vocab.get_vector(word)
            except:
              vector=None

            if vector is not None:
                cov_num += 1

            elif word in oov_dict.keys():
                oov_dict[word] += 1
                oov_num += 1

            else:
                oov_dict[word] = 1
                oov_num += 1

    oov_dict = {k:v for k, v in sorted(oov_dict.items(),
                                       key = lambda x:x[1],
                                       reverse = True)}

    print('Number of covered words: {}, \nNumber of uncovered wrods: {}'.format(cov_num, oov_num))
    print('Coverage precentage: {:.2f}'.format(cov_num /(cov_num + oov_num) *100))

    return oov_dict

oov = cov_check(df.Text, glove_model)
"
"""

#print('Most used, unrecognized words:\n', list(oov)[:1000])

"""def nonAlphabetRemoval(word):
  return ''.join(ch for ch in word if ch.isalnum() or ch is "'")

def clean(tweet):

    o = ''
    rep_redicilous_words = {
            "China\x89Ûªs":"china's", "let\x89Ûªs": "let's",
            "\x89ÛÏWhen":'when',"fromåÊwounds":'from wounds',
           "åÊ":o , "åÈ":o, "JapÌ_n":"Japan",
           "Ì©":o, "å¨":o, "SuruÌ¤":"Suruc",
          "åÇ":o, "å£3million":" a million",
           "åÀ":o, "he's":"he is", "there's":"there is",
          "We're":"We are", "That's":"That is",
           "won't":"will not", "They're":"They are",
           "Can't":"Can not",
          "wasn't":"was not", "don\x89Ûªt":"do not",
           "aren't":"are not", "isn't":"is not",
          "What's":"What is", "haven't":"have not",
           "hasn't":"has not", "There's":"There is",
          "He's":"He is", "It's":"It is",
           "You're":"You are", "I'M":"I am",
           "shouldn't":"should not",
          "wouldn't":"would not", "i'm":"I am", "Isn't":"is not",
          "Here's":"Here is", "you've":"you have",
           "you\x89Ûªve": "you have", "we're":"we are",
           "What's":"What is", "couln't":"could not",
           "we've": "we have", "it\x89Ûªs": "it is",
           "doesn\x89Ûªt": "does not", "It\x89Ûªs": "It is",
           "Here\x89Ûªs":"Here is", "Who's": "who is",
           "I\x89Ûªve": "I have", "y'all":"you all",
           "can\x89Ûªt": "cannot", "would've": "would have",
           "it'll":"it will", "we'll": "we will",
           "wouldn\x89Ûªt": "would not", "We've": "We have",
           "he'll": "he will", "Didn't": "Did not",
           "they'll": "they will", "they'd": "they would",
           "DON'T": "DO NOT", "That\x89Ûªs": "That is",
           "they've": "they have", "i'd": "I would",
           "should've": "should have", "You\x89Ûªre": "You are",
           "where's": "where is", "Don\x89Ûªt": "Do not",
           "we'd": "we would","i'll": "I will",
           "weren't": "were not", "They're": "They are",
           "Can\x89Ûªt": "Cannot", "you\x89Ûªll": "you will",
           "I\x89Ûªd": "I would", "let's": "let us",
           "it's": "it is", "can't": "cannot",
           "don't": "do not", "i've": "I have",
           "that's": "that is", "i'll": "I will",
           "doesn't": "does not", "i'd": "I would",
           "didn't": "did not", "ain't": "am not",
           "you'll":"you will", "I've": "I have",
           "Don't": "do not", "I'll": "I will",
           "I'd": "I would", "you'd": "You would",
           "Ain't": "am not", "donå«t": "do not",
           "ÏWhen":"When","ÏHatchet":"Hatchet",
            "CollisionNo":"Collision No"
        }


    tweet=tweet.replace('’',"'")
    # these have to be separatedly cleaned
    # because of their format
    tweet = re.sub(r"\x89û", "", tweet)
    tweet = re.sub(r"\x89Û", "", tweet)
    tweet = re.sub(r"\x89ã¢", "", tweet)
    tweet = re.sub(r"\x89Ûª", "", tweet)
    tweet = re.sub(r"\x89Û÷", "", tweet)
    tweet = re.sub(r"ª", "", tweet)
    tweet = re.sub(r"÷", "", tweet)
    tweet = re.sub(r"\x9d", "", tweet)
    tweet = re.sub(r"\x89ûò", "", tweet)
    tweet = re.sub(r"#USER#", "", tweet)
    tweet = re.sub(r"#URL#", "", tweet)
    tweet = re.sub('[^a-zA-Z0-9 \n\.]', "", tweet)
    tweet = re.sub(r'\w*\d\w*', '', tweet)






    # character_entity_refrences

    rep_character_entity_refrences = {"&gt;": ">", "&lt;":"<",
                                      "&amp;": "&"}

    # SLANG!

    slang_and_abbr = {
            "KurtSchlichter":"Kurt Schlichter","ProtectDenaliWolves":"Protect Denali Wolves", "fiancã": "fiancé", "gymrunningcyclingmtb" : "Gym running cycling MTB", "iâm": "i am",
            "iâve": "i have", "augustearly": "august early", "theyâre": "they are", "medidating": "meditating", "fifthgrade": "fifth grade",
            "plantbased": "plant based",  "ignredients": "ingredients",  "singersong": "singer song", "worldso": "world so", "walkinghiking": "walking hiking",  "placesbecause": "places because",  "timeam": "time am",
             "hobbiesbut": "hobbies but",  "songsbecause": "songs because", "myselfsinging": "myself singing", "campingit": "camping it", "countrymy": "country my", "beautifuli": "beautiful i", "foodloving": "food loving",
             "moviesthere": "movies there", "placesi": "places i", "tourat": "tour at"
             , "dancingit": "dancing it", "challengesit": "challenges it", "enthustiatics": "enthusiastic", "enjoymentand": "enjoyment and", "freshnesswatching": "freshness watching", "favorurite": "favourite",
             "hobbiemy": "hobbie my", "workmost": "work most", "instancebecause": "instance because", "enoromous": "enormous", "": "",
              "bowlingthis": "bowling this", "1gardening": "1 gardening", "2camping": "2 camping", "otherwiseif": "otherwise if", "workyes": "work yes", "personalityplaying": "personality playing", "lifebe": "life be",
              "nightlightgreat": "night light great", "beforeresorts": "before resorts", "craftsblogging": "crafts blogging", "selfknowledge": "self knowledge", "classeswhen": "classes when", "likebecause": "like because",
              "vacationperception": "vacation perception","neighborsduring": "neighbors during","articlesand": "articles and", "personin": "person in", "bodyusually": "body usually", "therelaxation": "the relaxation",
               "meevery": "me every", "lifetargets": "life targets"
              , "thingsi": "things i", "wouldnât": "would not", "relationsso": "relations so", "landmarksi": "landmarks i", "skillsdoing": "skills doing", "freshit": "fresh it"
              , "enjoyingand": "enjoying and", "hobbiessurfing": "hobbies surfing", "jobloved": "job loved", "hersleeping": "her sleeping", "napsleep": "nap sleep", "cookingi": "cooking i", "uniquei": "unique i"
              , "mediumbig": "medium big", "hothumid": "hot humid", "nightâs": "night ss", "depressionthis": "depression this", "issuesand": "issues and",
              "ourfears": "our fears", "lotthat": "lot that", "hobbiesit": "hobbies it","teamspirit": "team spirit", "activei": "active i","mouintains": "mountains","counsulting": "consulting",
              "vacationthat": "vacation that","enjoyfull": "enjoy full","relationsthere": "relations there","bodyso": "body so","sceenary": "scenery",
              "emphathise": "empathize","gardenscapes": "garden scapes","placetravel": "place travel","joyfueled": "joy fueled","everyonealso": "everyone also","everyonemade": "everyone made",
              "conditionit": "condition it","handeye": "hand eye","tvradio": "tv radio","bragworthy": "brag worthy", "lakesand": "lakes and", "dolphinwatchingthe": "dolphin watching the"
              , "planevisit": "plane visit", "poshmark": "posh mark", "bloglike": "blog like", "withthier": "with thier", "continuouly": "continuously",
              "touristless":"tourist less","regionvacation":"region vacation","hobbiues":"hobbies","tofarm":"to farm","nameit":"name it","mostevery":"most every","mostevery":"most every",
              "topicsi":"topics i","dancingi":"dancing i","sweetshobbies":"sweets hobbies","peacefuli":"peaceful i","wherehow":"where how","naturewildlife":"nature wild life",
              "othersi":"others i","orphansfestival":"orphans festival","traditionsi":"traditions i","conquences":"consequences","snaimals":"animals","aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa":"scream","aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa":"scream",
              "importantsea":"important sea","museumsmostly":"museums mostly","languageâs":"languages","naturecentered":"nature centered","ââgoing":"going","âœbubbleâ":"bubble",
              "frabic":"fabric", "nontourist":"non tourist","apsirations":"aspirations","inpersonate":"impersonate","doingfeeling":"doing feeling",
              "likewant":"like want","artside":"art side","bulletjournal":"bullet journal","naturei":"nature i","positivemostly":"positive mostly",
              "preferene":"preference","sericulature":"sericulture","analysethe":"analyse the","joini":"join i","vavation":"vacation","voicei":"voice i","activityrelaxation":"activity relaxation",
              "thinkinglearning":"thinking learning",
              "creativelogical":"creative logical","placecity":"place city","basicallyset":"basically set","studyinng":"studying","witner":"winter","qchord":"q chord","skiresort":"ski resort",
              "foodsi":"foods i",
              "dayswe":"days we","worksand":"works and","foozeball":"fooze ball","foodsi":"foods i", "hotelb":"hotel b", "localcaloric":"local caloric",
              "turkeyzanzibar":"turkey zanzibar","preferly":"preferably","solitudefilled":"solitude filled","turistical":"touristic","cultires":"cultures","citycountryisland":"city country island",
              "icandspend":"ic and spend","vacationtypically":"vacation typically","commeing":"comming","specifit":"specific","enjoysanibel":"enjoys anibel","areamy":"are amy"
              ,"teachie":"techie","seasond":"seasoned","remodal":"re modal","dreaminng":"dreaming","itlifts":"it lifts"
              ,"preper":"prepare","mediaparisians":"media parisians","driedup":"dried up","selfdiscipline":"self discipline"
              ,"sanctuaryâ":"sanctuary","roommatewhile":"roommate while","behaviori":"behavior","paticar":"playing"
                     }



    # adding all toghether and substituding

    for k,v in {**slang_and_abbr, **rep_redicilous_words,
                **rep_character_entity_refrences,}.items():

        tweet = re.sub(k, v, tweet)


    tweet=nltk.word_tokenize(tweet)
    tweet=' '.join([nonAlphabetRemoval(word) for word in tweet])

    # HTTP
    tweet = re.sub(r"https?:\/\/t.co\/[A-Za-z0-9]+", "", tweet)


    # PUNCTUATION

    tweet = tweet.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))


    stopwords = ["xxxxx", "xxxx", "xxx",'hashtag', 'url', 'user', "0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2", "a3", "a4", "ab", "able", "about", "above", "abst", "ac", "accordance", "according", "accordingly", "across", "act", "actually", "ad", "added", "adj", "ae", "af", "affected", "affecting", "affects", "after", "afterwards", "ag", "again", "against", "ah", "ain", "ain't", "aj", "al", "all", "allow", "allows", "almost", "alone", "along", "already", "also", "although", "always", "am", "among", "amongst", "amoungst", "amount", "an", "and", "announce", "another", "any", "anybody", "anyhow", "anymore", "anyone", "anything", "anyway", "anyways", "anywhere", "ao", "ap", "apart", "apparently", "appear", "appreciate", "appropriate", "approximately", "ar", "are", "aren", "arent", "aren't", "arise", "around", "as", "a's", "aside", "ask", "asking", "associated", "at", "au", "auth", "av", "available", "aw", "away", "awfully", "ax", "ay", "az", "b", "b1", "b2", "b3", "ba", "back", "bc", "bd", "be", "became", "because", "become", "becomes", "becoming", "been", "before", "beforehand", "begin", "beginning", "beginnings", "begins", "behind", "being", "believe", "below", "beside", "besides", "best", "better", "between", "beyond", "bi", "bill", "biol", "bj", "bk", "bl", "bn", "both", "bottom", "bp", "br", "brief", "briefly", "bs", "bt", "bu", "but", "bx", "by", "c", "c1", "c2", "c3", "ca", "call", "came", "can", "cannot", "cant", "can't", "cause", "causes", "cc", "cd", "ce", "certain", "certainly", "cf", "cg", "ch", "changes", "ci", "cit", "cj", "cl", "clearly", "cm", "c'mon", "cn", "co", "com", "come", "comes", "con", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn", "couldnt", "couldn't", "course", "cp", "cq", "cr", "cry", "cs", "c's", "ct", "cu", "currently", "cv", "cx", "cy", "cz", "d", "d2", "da", "date", "dc", "dd", "de", "definitely", "describe", "described", "despite", "detail", "df", "di", "did", "didn", "didn't", "different", "dj", "dk", "dl", "do", "does", "doesn", "doesn't", "doing", "don", "done", "don't", "down", "downwards", "dp", "dr", "ds", "dt", "du", "due", "during", "dx", "dy", "e", "e2", "e3", "ea", "each", "ec", "ed", "edu", "ee", "ef", "effect", "eg", "ei", "eight", "eighty", "either", "ej", "el", "eleven", "else", "elsewhere", "em", "empty", "en", "end", "ending", "enough", "entirely", "eo", "ep", "eq", "er", "es", "especially", "est", "et", "et-al", "etc", "eu", "ev", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere", "ex", "exactly", "example", "except", "ey", "f", "f2", "fa", "far", "fc", "few", "ff", "fi", "fifteen", "fifth", "fify", "fill", "find", "fire", "first", "five", "fix", "fj", "fl", "fn", "fo", "followed", "following", "follows", "for", "former", "formerly", "forth", "forty", "found", "four", "fr", "from", "front", "fs", "ft", "fu", "full", "further", "furthermore", "fy", "g", "ga", "gave", "ge", "get", "gets", "getting", "gi", "give", "given", "gives", "giving", "gj", "gl", "go", "goes", "going", "gone", "got", "gotten", "gr", "greetings", "gs", "gy", "h", "h2", "h3", "had", "hadn", "hadn't", "happens", "hardly", "has", "hasn", "hasnt", "hasn't", "have", "haven", "haven't", "having", "he", "hed", "he'd", "he'll", "hello", "help", "hence", "her", "here", "hereafter", "hereby", "herein", "heres", "here's", "hereupon", "hers", "herself", "hes", "he's", "hh", "hi", "hid", "him", "himself", "his", "hither", "hj", "ho", "home", "hopefully", "how", "howbeit", "however", "how's", "hr", "hs", "http", "hu", "hundred", "hy", "i", "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ignored", "ih", "ii", "ij", "il", "i'll", "im", "i'm", "immediate", "immediately", "importance", "important", "in", "inasmuch", "inc", "indeed", "index", "indicate", "indicated", "indicates", "information", "inner", "insofar", "instead", "interest", "into", "invention", "inward", "io", "ip", "iq", "ir", "is", "isn", "isn't", "it", "itd", "it'd", "it'll", "its", "it's", "itself", "iv", "i've", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "just", "k", "ke", "keep", "keeps", "kept", "kg", "kj", "km", "know", "known", "knows", "ko", "l", "l2", "la", "largely", "last", "lately", "later", "latter", "latterly", "lb", "lc", "le", "least", "les", "less", "lest", "let", "lets", "let's", "lf", "like", "liked", "likely", "line", "little", "lj", "ll", "ll", "ln", "lo", "look", "looking", "looks", "los", "lr", "ls", "lt", "ltd", "m", "m2", "ma", "made", "mainly", "make", "makes", "many", "may", "maybe", "me", "mean", "means", "meantime", "meanwhile", "merely", "mg", "might", "mightn", "mightn't", "mill", "million", "mine", "miss", "ml", "mn", "mo", "more", "moreover", "most", "mostly", "move", "mr", "mrs", "ms", "mt", "mu", "much", "mug", "must", "mustn", "mustn't", "my", "myself", "n", "n2", "na", "name", "namely", "nay", "nc", "nd", "ne", "near", "nearly", "necessarily", "necessary", "need", "needn", "needn't", "needs", "neither", "never", "nevertheless", "new", "next", "ng", "ni", "nine", "ninety", "nj", "nl", "nn", "no", "nobody", "non", "none", "nonetheless", "noone", "nor", "normally", "nos", "not", "noted", "nothing", "novel", "now", "nowhere", "nr", "ns", "nt", "ny", "o", "oa", "ob", "obtain", "obtained", "obviously", "oc", "od", "of", "off", "often", "og", "oh", "oi", "oj", "ok", "okay", "ol", "old", "om", "omitted", "on", "once", "one", "ones", "only", "onto", "oo", "op", "oq", "or", "ord", "os", "ot", "other", "others", "otherwise", "ou", "ought", "our", "ours", "ourselves", "out", "outside", "over", "overall", "ow", "owing", "own", "ox", "oz", "p", "p1", "p2", "p3", "page", "pagecount", "pages", "par", "part", "particular", "particularly", "pas", "past", "pc", "pd", "pe", "per", "perhaps", "pf", "ph", "pi", "pj", "pk", "pl", "placed", "please", "plus", "pm", "pn", "po", "poorly", "possible", "possibly", "potentially", "pp", "pq", "pr", "predominantly", "present", "presumably", "previously", "primarily", "probably", "promptly", "proud", "provides", "ps", "pt", "pu", "put", "py", "q", "qj", "qu", "que", "quickly", "quite", "qv", "r", "r2", "ra", "ran", "rather", "rc", "rd", "re", "readily", "really", "reasonably", "recent", "recently", "ref", "refs", "regarding", "regardless", "regards", "related", "relatively", "research", "research-articl", "respectively", "resulted", "resulting", "results", "rf", "rh", "ri", "right", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "said", "same", "saw", "say", "saying", "says", "sc", "sd", "se", "sec", "second", "secondly", "section", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "sf", "shall", "shan", "shan't", "she", "shed", "she'd", "she'll", "shes", "she's", "should", "shouldn", "shouldn't", "should've", "show", "showed", "shown", "showns", "shows", "si", "side", "significant", "significantly", "similar", "similarly", "since", "sincere", "six", "sixty", "sj", "sl", "slightly", "sm", "sn", "so", "some", "somebody", "somehow", "someone", "somethan", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "sp", "specifically", "specified", "specify", "specifying", "sq", "sr", "ss", "st", "still", "stop", "strongly", "sub", "substantially", "successfully", "such", "sufficiently", "suggest", "sup", "sure", "sy", "system", "sz", "t", "t1", "t2", "t3", "take", "taken", "taking", "tb", "tc", "td", "te", "tell", "ten", "tends", "tf", "th", "than", "thank", "thanks", "thanx", "that", "that'll", "thats", "that's", "that've", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "thered", "therefore", "therein", "there'll", "thereof", "therere", "theres", "there's", "thereto", "thereupon", "there've", "these", "they", "theyd", "they'd", "they'll", "theyre", "they're", "they've", "thickv", "thin", "think", "third", "this", "thorough", "thoroughly", "those", "thou", "though", "thoughh", "thousand", "three", "throug", "through", "throughout", "thru", "thus", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "together", "too", "took", "top", "toward", "towards", "tp", "tq", "tr", "tried", "tries", "truly", "try", "trying", "ts", "t's", "tt", "tv", "twelve", "twenty", "twice", "two", "tx", "u", "u201d", "ue", "ui", "uj", "uk", "um", "un", "under", "unfortunately", "unless", "unlike", "unlikely", "until", "unto", "uo", "up", "upon", "ups", "ur", "us", "use", "used", "useful", "usefully", "usefulness", "uses", "using", "usually", "ut", "v", "va", "value", "various", "vd", "ve", "ve", "very", "via", "viz", "vj", "vo", "vol", "vols", "volumtype", "vq", "vs", "vt", "vu", "w", "wa", "want", "wants", "was", "wasn", "wasnt", "wasn't", "way", "we", "wed", "we'd", "welcome", "well", "we'll", "well-b", "went", "were", "we're", "weren", "werent", "weren't", "we've", "what", "whatever", "what'll", "whats", "what's", "when", "whence", "whenever", "when's", "where", "whereafter", "whereas", "whereby", "wherein", "wheres", "where's", "whereupon", "wherever", "whether", "which", "while", "whim", "whither", "who", "whod", "whoever", "whole", "who'll", "whom", "whomever", "whos", "who's", "whose", "why", "why's", "wi", "widely", "will", "willing", "wish", "with", "within", "without", "wo", "won", "wonder", "wont", "won't", "words", "world", "would", "wouldn", "wouldnt", "wouldn't", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx", "y", "y2", "yes", "yet", "yj", "yl", "you", "youd", "you'd", "you'll", "your", "youre", "you're", "yours", "yourself", "yourselves", "you've", "yr", "ys", "yt", "z", "zero", "zi", "zz"]
    querywords = tweet.split()

    resultwords  = [word for word in querywords if word.lower() not in stopwords]
    tweet = ' '.join(resultwords)



    return tweet.lower()

df['Text'] = df.Text.apply(lambda x : clean(x))

oov = cov_check(df['Text'], glove_model)

print('Most used, unrecognized words:\n', list(oov)[:9999])
"""

#len(oov)

"""# Further cleaning"""

#Load it and start from here
#df = pd.read_csv('/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/Personality_twitter_clean.csv')

# Remove the stopwords, unknown words and the user and hashtag from the text #hashtag #url #user # info
"""def clean_further(tweet, oov):
    counter = 0
    querywords = tweet.split()
    resultwords  = [word for word in querywords if word.lower() not in oov]
    tweet = ' '.join(resultwords)
    #counter = counter+1
    #print(counter)
    return tweet.lower()

df['Text'] = df.Text.apply(lambda x : clean_further(x, list(oov)[190000:206196]))"""

"""#Save Clean df and load it again"""

# Save it only one time
#df.to_csv("/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/Personality_twitter_clean.csv", index=False)

"""#Start from here"""

df = pd.read_csv('/content/drive/MyDrive/ambiversion files/Personality_twitter_clean.csv')



df.head()

type(df["Text"][1])

#Change the length of the string
df['Text'] = df['Text'].str[:3500]

df["Text"][2]

def set_MBTI_exp1(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP":
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENTJ":
        return 2


def set_MBTI_exp2(row):
    if row["MBTI"] == "INFP" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP"or row["MBTI"] == "ENTJ" or row["MBTI"] == "ESTJ"or row["MBTI"] == "ENFJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or  row["MBTI"] == "ISTJ" :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP":
        return 2



def set_MBTI_exp4(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTJ" or row["MBTI"] == "ENTP"or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  :
        return 2

def set_MBTI_exp5(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP" or row["MBTI"] == "ENTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ" :
        return 2

def set_MBTI_exp6(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP"or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ"  :
        return 2

def set_MBTI_exp7(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP" or row["MBTI"] == "ENTJ"or row["MBTI"] == "ENFJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  or row["MBTI"] == "ESTJ" :
        return 2

df1 = df.assign(label=df.apply(set_MBTI_exp1, axis=1))
df2 = df.assign(label=df.apply(set_MBTI_exp2, axis=1))
df4 = df.assign(label=df.apply(set_MBTI_exp4, axis=1))
df5 = df.assign(label=df.apply(set_MBTI_exp5, axis=1))
df6 = df.assign(label=df.apply(set_MBTI_exp6, axis=1))
df7 = df.assign(label=df.apply(set_MBTI_exp7, axis=1))
print(df1)

def set_MBTI_exp3(row):
  if  row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP":
    return 0
  elif row["MBTI"] == "INFJ" or row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP":
    return 1
  elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  or row["MBTI"] == "ENTJ" or row["MBTI"] == "ENTP" or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENFJ":
    return 2

    # -  -  -  - -  -

df3 = df.assign(label=df.apply(set_MBTI_exp3, axis=1))

"""# Set target and Class names"""

target_names = [0,1,2]
class_names = ['Middle Group', 'Introvert', 'Extrovert']

print("DF 1", "\n",  df1.label.value_counts())
print("DF 2", "\n",  df2.label.value_counts())
print("DF 3", "\n",  df3.label.value_counts())
print("DF 4", "\n",  df4.label.value_counts())
print("DF 5", "\n",  df5.label.value_counts())
print("DF 6", "\n",  df6.label.value_counts())
print("DF 7", "\n",  df7.label.value_counts())

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion Matrix',
                          cmap=None,
                          normalize=True):
    """
    given a sklearn confusion matrix (cm), make a nice plot

    Arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html
                  plt.get_cmap('jet') or plt.cm.Blues

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions

    Usage
    -----
    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by
                                                              # sklearn.metrics.confusion_matrix
                          normalize    = True,                # show proportions
                          target_names = y_labels_vals,       # list of names of the classes
                          title        = best_estimator_name) # title of graph

    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(6, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)

    plt.title(title)
    plt.colorbar()
    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label (avg)')
    plt.xlabel('Predicted label (avg)\naccuracy (avg)={:0.4f}; misclass (avg)={:0.4f}'.format(accuracy, misclass))
    plt.grid(None)
    plt.grid(b=None)
    plt.figure( figsize=(20,10) )
    plt.show()

def plot_graph(epochs,train_loss,valid_loss):
    fig = plt.figure(figsize=(12,12))
    plt.title("Train/Validation Loss")
    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')
    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='validation')
    plt.xlabel('num_epochs', fontsize=12)
    plt.ylabel('loss', fontsize=12)
    plt.legend(loc='best')

import pandas as pd
import re
import string
import numpy as np
import matplotlib.pyplot as plt


from sklearn.metrics import roc_curve
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import f1_score, accuracy_score,classification_report
from sklearn.model_selection import train_test_split
from simpletransformers.classification import ClassificationModel


from simpletransformers.classification import (
    MultiLabelClassificationModel, MultiLabelClassificationArgs
)
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

def f1_multiclass(labels, preds):
    #print(classification_report(labels, preds),cm(labels, preds))
    return preds

import os
import shutil
import torch

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report

def run_ModelTransformer(transformermodel,fname, dframe, X, y,algoname= ' ', _target_names = [], _class_names = []):

    #kfold = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)
    cfmean = []
    accmean = []
    Wprecmean = []
    Wrecmean = []
    Wf1mean = []

    Mprecmean = []
    Mrecmean = []
    Mf1mean = []


    X_test_mean = []
    y_test_mean = []
    y_pred_mean = []

    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)
    #fig, ax = plt.subplots(figsize=(9, 9))

    count = 1
    """for i, (train_index, test_index) in enumerate(kfold.split(X, y)):
        train_X, test_X = X[train_index], X[test_index]
        train_y, test_y = y[train_index], y[test_index]


        train=pd.DataFrame()
        train['text']=train_X
        train['label']=train_y
        print(train.label.value_counts())


        test=pd.DataFrame()
        test['text']=test_X
        test['label']=test_y
        print(test.label.value_counts())"""

    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, stratify=dframe['MBTI'])
    train=pd.DataFrame()
    train['text']=train_X
    train['label']=train_y
    print(train.label.value_counts())


    test=pd.DataFrame()
    test['text']=test_X
    test['label']=test_y
    print(test.label.value_counts())

    cuda_available = torch.cuda.is_available()



    # Create a TransformerModel
    model = ClassificationModel(transformermodel[0], transformermodel[1],num_labels=3,ignore_mismatched_sizes=True, use_cuda=cuda_available, args={'num_train_epochs': 28, 'learning_rate': 6e-5,'max_seq_length': 512, 'use_multiprocessing' : False, 'use_multiprocessing_for_evaluation' : False })#learning_rate': 3e-5  #4096

    # Train the model
    model.train_model(train)


    result, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score)
    y_pred=result['f1']
    y_true=test_y

    test['predicted']=y_pred
    test.to_csv('/content/drive/MyDrive/ambiversion files/baslines'+str(fname)+'.csv',index=None)



    #X_test_mean.append(X_test)
    y_test_mean.append(y_true)
    y_pred_mean.append(y_pred)




    print("Iteration:" + str(count))

    print("=== Scores ===")
    accuracy = accuracy_score(y_true, y_pred)
    print('Accuracy: %f' % accuracy)
    accmean.append(accuracy)


    precision = precision_score(y_true, y_pred,average='weighted')
    recall = recall_score(y_true, y_pred,average='weighted')
    f1 = f1_score(y_true, y_pred,average='weighted')
    print('weighted scores:\tprecision: ',precision,'    recall: ',recall,'    F1-score: ',f1)

    #Wprecmean.append(precision)
    #Wrecmean.append(recall)
    #Wf1mean.append(f1)




    precision = precision_score(y_true, y_pred,average='macro')
    recall = recall_score(y_true, y_pred,average='macro')
    f1 = f1_score(y_true, y_pred,average='macro')
    print('macro scores:\tprecision: ',precision,'    recall: ',recall,'    F1-score: ',f1)

    Mprecmean.append(precision)
    Mrecmean.append(recall)
    Mf1mean.append(f1)


    print("=== Classification Report ===")
    target_names = _target_names
    print(classification_report(y_true, y_pred, target_names=target_names))

    print("=== Confusion Matrix ===")
    print(confusion_matrix(y_true, y_pred))
    cfmean.append(confusion_matrix(y_true, y_pred))

    print('\n')
    count = count + 1
    shutil.rmtree('/content/outputs')#to save space



    acc = np.mean(accmean, axis=0)
    cm = np.mean(cfmean, axis=0)

    Wpre = np.mean(Wprecmean, axis=0)
    Wre = np.mean(Wrecmean, axis=0)
    Wf1 = np.mean(Wf1mean, axis=0)

    Mpre = np.mean(Mprecmean, axis=0)
    Mre = np.mean(Mrecmean, axis=0)
    Mf1 = np.mean(Mf1mean, axis=0)

    #print('\n')
    #print("Averages\n")
    #print('acuuracy: ',acc)

    #print('Weighted scores:\tprecision: ',Wpre,'    recall: ',Wre,'    F1-score: ',Wf1)
    #print('Macro scores:\tprecision: ',Mpre,'    recall: ',Mre,'    F1-score: ',Mf1)

    #print('Confusion Matrix\n')
    #print(cm)

    #print('Classification report:\n')
    #print(classification_report(y_test_mean, y_pred_mean, target_names=target_names))




    #plot graph
    #class_names = _class_names
    #plot_confusion_matrix(cm, target_names=class_names,title='')
    #plt.show()







    #return clf, kfold

!rm -r /content/outputs

#filename = "Experiment1"
#run_ModelTransformer(['longformer', 'allenai/longformer-base-4096'], filename, df1, df1.Text, df1.label, target_names, class_names)
filename = "Experiment1"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df1, df1.Text, df1.label, target_names, class_names)

filename = "Experiment2"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df2, df2.Text, df2.label, target_names, class_names)

filename = "Experiment3"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df3, df3.Text, df3.label, target_names, class_names)

filename = "Experiment4"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df4, df4.Text, df4.label, target_names, class_names)

filename = "Experiment5"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df5, df5.Text, df5.label, target_names, class_names)

filename = "Experiment6"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df6, df6.Text, df6.label, target_names, class_names)

filename = "Experiment7"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df7, df7.Text, df7.label, target_names, class_names)

"""#Other transformers"""

filename = "RobertaExperiment2"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "RobertaExperiment3"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "RobertaExperiment4"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df4,df4.Text, df4.label, target_names, class_names)

"""#ALBERT"""

filename = "AlbertExperiment2"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "AlbertExperiment3"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "AlbertExperiment4"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df4,df4.Text, df4.label, target_names, class_names)

"""#XLNET"""

filename = "XlnetExperiment2"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "XlnetExperiment3"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "XlnetExperiment4"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df4,df4.Text, df4.label, target_names, class_names)

"""#ELECTRA"""

filename = "ElectraExperiment2"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "ElectraExperiment3"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "ElectraExperiment4"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df4,df4.Text, df4.label, target_names, class_names)

"""#DistilBERT"""

filename = "DistilExperiment2"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "DistilExperiment3"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "DistilExperiment4"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df4,df4.Text, df4.label, target_names, class_names)