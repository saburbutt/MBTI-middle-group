# -*- coding: utf-8 -*-
"""(Without pre-processing) Transformer_baselines_Twitter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wHZJhbYPrwVOgKcVGQhd-dIWncvakMgg
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers
# !pip install tensorboardx
# !pip install simpletransformers

import csv
import os
import re
import time
import pickle
import string
import spacy
import nltk
import warnings
import random

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
'''import tensorflow as tf

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer'''
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn import metrics
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score,fbeta_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.pipeline import Pipeline
#from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
'''from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import text_to_word_sequence
from keras.preprocessing import sequence
from keras import backend
from keras import backend as K
from keras import models
from keras.models import Sequential
from keras.layers import Dense,LSTM, Dropout, Flatten, Embedding, Bidirectional, GlobalAveragePooling1D
from keras.layers import Conv1D,Conv2D, MaxPooling2D, MaxPooling1D, GlobalMaxPooling1D
#from keras.optimizers import Adam,Adadelta,Adagrad,Adamax
from keras.regularizers import l2,l1
from keras.callbacks import CSVLogger
#from keras.utils import to_categorical
from keras.datasets import mnist
from keras.utils.vis_utils import model_to_dot
#from keras.layers.normalization import BatchNormalization
from keras.layers import Conv1D,Conv2D, MaxPooling2D, MaxPooling1D
from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
from IPython.display import SVG
from scipy.stats import norm
from gensim import models
from gensim.models import Word2Vec
from gensim.models.wrappers import FastText
from gensim.models.fasttext import FastText#, load_facebook_vectors
from gensim.models.keyedvectors import KeyedVectors'''
from tqdm import tqdm
from nltk.corpus import wordnet
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, PorterStemmer ,WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')


warnings.filterwarnings("ignore")

#f =  open("/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/2000gb.all", 'rb')
#full_text = f.readlines()
#f.close()
#decode('string_escape')
#for text in full_text[0:1]:
#    print(str(text))
#    print(type(text))
#    print("----------------------------------------------")

"""#Data Statistics"""

# Read TSV file into DataFrame
df = pd.read_table('/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/2000gb.all', sep='\t', encoding='latin-1', header=None,)
print(df)

df.head()

# Save it only one time
# df.to_csv("/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/Personality_twitter_unclean.csv", index=False)

# adding column name to the respective columns
df.columns =['MBTI', 'Gender', 'Number', 'Text']

df["MBTI"].unique()

df[["MBTI", "Gender"]].describe()

# counting unique values
df['MBTI'].value_counts()

print(df.shape)

"""#Data transformatrion

# Further cleaning
"""

#df = pd.read_csv('/content/drive/MyDrive/Thesis-Ambiversion/MBTI-Twitter/Personality_twitter_unclean.csv')

df.head()

type(df["Text"][1])

#Change the length of the string
df['Text'] = df['Text'].str[:4000]

df["Text"][2]

def set_MBTI_exp1(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP":
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENTJ":
        return 2


def set_MBTI_exp2(row):
    if row["MBTI"] == "INFP" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP"or row["MBTI"] == "ENTJ" or row["MBTI"] == "ESTJ"or row["MBTI"] == "ENFJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or  row["MBTI"] == "ISTJ" :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP":
        return 2



def set_MBTI_exp4(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTJ" or row["MBTI"] == "ENTP"or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  :
        return 2

def set_MBTI_exp5(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP" or row["MBTI"] == "ENTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ" or row["MBTI"] == "ESTJ" :
        return 2

def set_MBTI_exp6(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP"or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENTJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP" or row["MBTI"] == "ENFJ"  :
        return 2

def set_MBTI_exp7(row):
    if row["MBTI"] == "INFJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP" or row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP" or row["MBTI"] == "ENTP" or row["MBTI"] == "ENTJ"or row["MBTI"] == "ENFJ":
        return 0
    elif row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ"  :
        return 1
    elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  or row["MBTI"] == "ESTJ" :
        return 2

df1 = df.assign(label=df.apply(set_MBTI_exp1, axis=1))
df2 = df.assign(label=df.apply(set_MBTI_exp2, axis=1))
df4 = df.assign(label=df.apply(set_MBTI_exp4, axis=1))
df5 = df.assign(label=df.apply(set_MBTI_exp5, axis=1))
df6 = df.assign(label=df.apply(set_MBTI_exp6, axis=1))
df7 = df.assign(label=df.apply(set_MBTI_exp7, axis=1))
print(df1)

def set_MBTI_exp3(row):
  if  row["MBTI"] == "ISFJ" or row["MBTI"] == "ENFP":
    return 0
  elif row["MBTI"] == "INFJ" or row["MBTI"] == "INTJ" or row["MBTI"] == "INTP" or row["MBTI"] == "INFP" or row["MBTI"] == "ISTJ" or row["MBTI"] == "ISFP" or row["MBTI"] == "ISTP":
    return 1
  elif row["MBTI"] == "ESFP" or row["MBTI"] == "ESFJ" or row["MBTI"] == "ESTP"  or row["MBTI"] == "ENTJ" or row["MBTI"] == "ENTP" or row["MBTI"] == "ESTJ" or row["MBTI"] == "ENFJ":
    return 2

    # -  -  -  - -  -

df3 = df.assign(label=df.apply(set_MBTI_exp3, axis=1))

"""# Set target and Class names"""

target_names = [0,1,2]
class_names = ['Middle Group', 'Introvert', 'Extrovert']

print("DF 1", "\n",  df1.label.value_counts())
print("DF 2", "\n",  df2.label.value_counts())
print("DF 3", "\n",  df3.label.value_counts())
print("DF 4", "\n",  df4.label.value_counts())
print("DF 5", "\n",  df5.label.value_counts())
print("DF 6", "\n",  df6.label.value_counts())
print("DF 7", "\n",  df7.label.value_counts())

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion Matrix',
                          cmap=None,
                          normalize=True):
    """
    given a sklearn confusion matrix (cm), make a nice plot

    Arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html
                  plt.get_cmap('jet') or plt.cm.Blues

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions

    Usage
    -----
    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by
                                                              # sklearn.metrics.confusion_matrix
                          normalize    = True,                # show proportions
                          target_names = y_labels_vals,       # list of names of the classes
                          title        = best_estimator_name) # title of graph

    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(6, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)

    plt.title(title)
    plt.colorbar()
    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label (avg)')
    plt.xlabel('Predicted label (avg)\naccuracy (avg)={:0.4f}; misclass (avg)={:0.4f}'.format(accuracy, misclass))
    plt.grid(None)
    plt.grid(b=None)
    plt.figure( figsize=(20,10) )
    plt.show()

def plot_graph(epochs,train_loss,valid_loss):
    fig = plt.figure(figsize=(12,12))
    plt.title("Train/Validation Loss")
    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')
    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='validation')
    plt.xlabel('num_epochs', fontsize=12)
    plt.ylabel('loss', fontsize=12)
    plt.legend(loc='best')

import pandas as pd
import re
import string
import numpy as np
import matplotlib.pyplot as plt


from sklearn.metrics import roc_curve
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import f1_score, accuracy_score,classification_report
from sklearn.model_selection import train_test_split
from simpletransformers.classification import ClassificationModel


from simpletransformers.classification import (
    MultiLabelClassificationModel, MultiLabelClassificationArgs
)
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

def f1_multiclass(labels, preds):
    #print(classification_report(labels, preds),cm(labels, preds))
    return preds

import os
import shutil
import torch

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import pandas as pd
import torch
from simpletransformers.classification import ClassificationModel
import shutil

def run_ModelTransformer(transformermodel, fname, dframe, X, y, algoname=' ', _target_names=[], _class_names=[]):
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)
    cfmean = []
    accmean = []
    Wprecmean = []
    Wrecmean = []
    Wf1mean = []

    Mprecmean = []
    Mrecmean = []
    Mf1mean = []

    y_test_mean = []
    y_pred_mean = []

    count = 1

    # Compute class weights as a list
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weights = class_weights.tolist()  # Convert to list

    for i, (train_index, test_index) in enumerate(kfold.split(X, y)):
        train_X, test_X = X[train_index], X[test_index]
        train_y, test_y = y[train_index], y[test_index]

        train = pd.DataFrame()
        train['text'] = train_X
        train['label'] = train_y
        print(train.label.value_counts())

        test = pd.DataFrame()
        test['text'] = test_X
        test['label'] = test_y
        print(test.label.value_counts())

        cuda_available = torch.cuda.is_available()

        # Create a TransformerModel with class weights
        model = ClassificationModel(
            transformermodel[0],
            transformermodel[1],
            num_labels=len(np.unique(y)),
            weight=class_weights,  # Pass the class weights as a list
            ignore_mismatched_sizes=True,
            use_cuda=cuda_available,
            args={
                'num_train_epochs': 30,
                'learning_rate': 6e-5,
                'max_seq_length': 512,
                'use_multiprocessing': False,
                'use_multiprocessing_for_evaluation': False,
                'overwrite_output_dir': True  # Allow overwriting of the output directory
            }
        )

        # Train the model
        model.train_model(train)

        # Evaluate the model
        result, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score)
        y_pred = result['f1']
        y_true = test_y

        test['predicted'] = y_pred
        test.to_csv('/content/drive/MyDrive/Thesis-Ambiversion/Baselines_new/'+str(fname)+'.csv', index=None)

        y_test_mean.append(y_true)
        y_pred_mean.append(y_pred)

        print("Iteration:" + str(count))

        print("=== Scores ===")
        accuracy = accuracy_score(y_true, y_pred)
        print('Accuracy: %f' % accuracy)
        accmean.append(accuracy)

        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
        print('Weighted scores:\tprecision: ', precision, '    recall: ', recall, '    F1-score: ', f1)

        Wprecmean.append(precision)
        Wrecmean.append(recall)
        Wf1mean.append(f1)

        precision = precision_score(y_true, y_pred, average='macro')
        recall = recall_score(y_true, y_pred, average='macro')
        f1 = f1_score(y_true, y_pred, average='macro')
        print('Macro scores:\tprecision: ', precision, '    recall: ', recall, '    F1-score: ', f1)

        Mprecmean.append(precision)
        Mrecmean.append(recall)
        Mf1mean.append(f1)

        print("=== Classification Report ===")
        target_names = _target_names
        print(classification_report(y_true, y_pred, target_names=target_names))

        print("=== Confusion Matrix ===")
        print(confusion_matrix(y_true, y_pred))
        cfmean.append(confusion_matrix(y_true, y_pred))

        print('\n')
        count = count + 1
        shutil.rmtree('/content/outputs')  # to save space

    acc = np.mean(accmean, axis=0)
    cm = np.mean(cfmean, axis=0)

    Wpre = np.mean(Wprecmean, axis=0)
    Wre = np.mean(Wrecmean, axis=0)
    Wf1 = np.mean(Wf1mean, axis=0)

    Mpre = np.mean(Mprecmean, axis=0)
    Mre = np.mean(Mrecmean, axis=0)
    Mf1 = np.mean(Mf1mean, axis=0)

    print('\n')
    print("Averages\n")
    print('Accuracy: ', acc)
    print('Weighted scores:\tprecision: ', Wpre, '    recall: ', Wre, '    F1-score: ', Wf1)
    print('Macro scores:\tprecision: ', Mpre, '    recall: ', Mre, '    F1-score: ', Mf1)

    print('Confusion Matrix\n')
    print(cm)

    #print('Classification report:\n')
    #print(classification_report(y_test_mean, y_pred_mean, target_names=target_names))

!rm -r /content/outputs

#filename = "Experiment1"
#run_ModelTransformer(['longformer', 'allenai/longformer-base-4096'], filename, df1, df1.Text, df1.label, target_names, class_names)
#filename = "Experiment1"
#run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df1, df1.Text, df1.label, target_names, class_names)

filename = "Experiment2"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df2, df2.Text, df2.label, target_names, class_names)

filename = "Experiment3"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df3, df3.Text, df3.label, target_names, class_names)

filename = "Experiment4"
run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df4, df4.Text, df4.label, target_names, class_names)

#filename = "Experiment5"
#run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df5, df5.Text, df5.label, target_names, class_names)

#filename = "Experiment6"
#run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df6, df6.Text, df6.label, target_names, class_names)

#filename = "Experiment7"
#run_ModelTransformer(['bert', 'bert-base-uncased'], filename, df7, df7.Text, df7.label, target_names, class_names)

"""#Other transformers"""

filename = "RobertaExperiment2"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "RobertaExperiment3"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "RobertaExperiment4"
run_ModelTransformer(['roberta', 'roberta-base'],filename,df4,df4.Text, df4.label, target_names, class_names)

"""#ALBERT"""

filename = "AlbertExperiment2"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "AlbertExperiment3"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "AlbertExperiment4"
run_ModelTransformer(['albert', 'albert-base-v2'], filename,df4,df4.Text, df4.label, target_names, class_names)

"""#XLNET"""

filename = "XlnetExperiment2"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "XlnetExperiment3"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "XlnetExperiment4"
run_ModelTransformer(['xlnet', 'xlnet-base-cased'],filename,df4,df4.Text, df4.label, target_names, class_names)

"""#ELECTRA"""

filename = "ElectraExperiment2"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "ElectraExperiment3"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "ElectraExperiment4"
run_ModelTransformer(['electra', 'google/electra-base-generator'], filename,df4,df4.Text, df4.label, target_names, class_names)

"""#DistilBERT"""

filename = "DistilExperiment2"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df2,df2.Text, df2.label, target_names, class_names)

filename = "DistilExperiment3"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df3,df3.Text, df3.label, target_names, class_names)

filename = "DistilExperiment4"
run_ModelTransformer(['distilbert', 'distilbert-base-uncased'], filename,df4,df4.Text, df4.label, target_names, class_names)